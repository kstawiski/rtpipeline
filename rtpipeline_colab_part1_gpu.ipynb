{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTpipeline on Google Colab - Part 1: GPU Segmentation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kstawiski/rtpipeline/blob/main/rtpipeline_colab_part1_gpu.ipynb)\n",
    "\n",
    "**üí∞ Cost Optimization:** This notebook is split into two parts to optimize GPU costs:\n",
    "- **Part 1 (this notebook):** Runs TotalSegmentator with GPU (~10-30 min/patient)\n",
    "- **Part 2:** Runs DVH, radiomics, and analysis on CPU only (saves GPU costs)\n",
    "\n",
    "## What This Part Does\n",
    "\n",
    "‚úÖ **Automatic segmentation** of 100+ organs using TotalSegmentator (GPU-accelerated)\n",
    "‚úÖ **Saves outputs** to Google Drive for Part 2\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Colab with **GPU runtime** (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- DICOM files in Google Drive\n",
    "- Google Drive mounted for saving outputs\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö° Quick Start:** \n",
    "1. Run cells 1-3 (setup)\n",
    "2. Mount Google Drive (cell 4)\n",
    "3. **UPDATE CONFIGURATION** (cell 5) - Point to your DICOM folder\n",
    "4. Run remaining cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Install Miniconda & System Dependencies\n",
    "\n",
    "This takes ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check GPU availability\n",
    "echo \"=== GPU Check ===\"\n",
    "nvidia-smi || echo \"‚ö†Ô∏è No GPU detected. Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\"\n",
    "\n",
    "# Install system dependencies\n",
    "echo -e \"\\n=== Installing System Dependencies ===\"\n",
    "apt-get update -qq\n",
    "apt-get install -y -qq dcm2niix pigz > /dev/null\n",
    "\n",
    "# Install Miniconda if not already installed\n",
    "if [ ! -d \"/content/miniconda\" ]; then\n",
    "    echo -e \"\\n=== Installing Miniconda ===\"\n",
    "    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n",
    "    bash /tmp/miniconda.sh -b -p /content/miniconda\n",
    "    rm /tmp/miniconda.sh\n",
    "    echo \"‚úÖ Miniconda installed\"\n",
    "else\n",
    "    echo \"‚úÖ Miniconda already installed\"\n",
    "fi\n",
    "\n",
    "# Initialize conda\n",
    "export PATH=\"/content/miniconda/bin:$PATH\"\n",
    "eval \"$(/content/miniconda/bin/conda shell.bash hook)\"\n",
    "conda init bash\n",
    "\n",
    "echo -e \"\\n‚úÖ Setup complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone RTpipeline Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"/content/rtpipeline\" ]; then\n",
    "    echo \"Cloning rtpipeline repository...\"\n",
    "    git clone -q https://github.com/kstawiski/rtpipeline.git /content/rtpipeline\n",
    "    echo \"‚úÖ Repository cloned\"\n",
    "else\n",
    "    echo \"‚úÖ Repository already exists\"\n",
    "    cd /content/rtpipeline\n",
    "    git pull origin main\n",
    "    echo \"Repository updated\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Conda Environment\n",
    "\n",
    "This creates the rtpipeline environment for TotalSegmentator (~5-10 minutes, only once per session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"/content/miniconda/bin:$PATH\"\n",
    "eval \"$(/content/miniconda/bin/conda shell.bash hook)\"\n",
    "\n",
    "# Accept Anaconda Terms of Service\n",
    "echo \"=== Accepting Anaconda Terms of Service ===\"\n",
    "conda config --set channel_priority flexible\n",
    "if ! conda tos accept --channel defaults 2>&1; then\n",
    "    echo \"‚ö†Ô∏è ToS acceptance failed or already accepted\"\n",
    "fi\n",
    "echo \"‚úÖ ToS accepted\"\n",
    "\n",
    "cd /content/rtpipeline\n",
    "\n",
    "# Create rtpipeline environment\n",
    "if conda env list | grep -q \"^rtpipeline \"; then\n",
    "    echo \"‚úÖ Environment 'rtpipeline' already exists\"\n",
    "else\n",
    "    echo \"Creating 'rtpipeline' environment (TotalSegmentator)...\"\n",
    "    conda env create -f envs/rtpipeline.yaml -q\n",
    "    echo \"‚úÖ Environment created\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "conda run -n rtpipeline python -c \"import numpy; print(f'‚úÖ numpy {numpy.__version__}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Mount Google Drive\n",
    "\n",
    "**IMPORTANT:** Your DICOM files must be in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted at /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURATION - UPDATE THIS!\n",
    "\n",
    "## 5Ô∏è‚É£ Configure Input/Output Paths & Processing Options\n",
    "\n",
    "**üî¥ REQUIRED:** Update `DICOM_ROOT` to point to your DICOM files in Google Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üî¥ REQUIRED - Point to your DICOM folder in Google Drive\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "DICOM_ROOT = \"/content/drive/MyDrive/my_dicom_folder\"\n",
    "\n",
    "# Examples:\n",
    "# DICOM_ROOT = \"/content/drive/MyDrive/RT_Data/DICOM\"\n",
    "# DICOM_ROOT = \"/content/drive/MyDrive/Patient_Data\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Output location in Google Drive (for Part 2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/rtpipeline_part1_output\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Parallelism & Performance Settings\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Course-level parallelism (how many courses to process simultaneously)\n",
    "# Google Colab: Keep at 1-2 to avoid memory issues with GPU\n",
    "# For multiple small patients, can try 2-3\n",
    "WORKERS = 1  # Recommended: 1-2 for Colab GPU\n",
    "\n",
    "# GPU worker allocation (keep at 1 for single GPU Colab)\n",
    "SEG_WORKERS = 1  # DO NOT CHANGE (only 1 GPU available in Colab)\n",
    "\n",
    "# TotalSegmentator internal threading\n",
    "# These control parallelism WITHIN each segmentation task\n",
    "# Higher values = faster but more memory usage\n",
    "TOTALSEG_NR_THR_RESAMP = 1      # Resampling threads (1-2 recommended)\n",
    "TOTALSEG_NR_THR_SAVING = 6      # I/O threads for saving (4-8 recommended)\n",
    "TOTALSEG_NUM_PROC_PRE = 6       # Preprocessing processes (4-8 recommended)\n",
    "TOTALSEG_NUM_PROC_EXPORT = 6    # Export processes (4-8 recommended)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Segmentation Options\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Fast mode: 3x faster but slightly lower quality\n",
    "FAST_MODE = False  # Set to True for faster segmentation\n",
    "\n",
    "# ROI subset: segment only specific organs (leave None for all)\n",
    "# Examples: \"liver kidney spleen\", \"lung_left lung_right\"\n",
    "# See TotalSegmentator docs for available ROI names\n",
    "ROI_SUBSET = None  # None = segment all organs\n",
    "\n",
    "# Extra TotalSegmentator models (body composition, cardiac, etc.)\n",
    "# Available: \"body\", \"lung_vessels\", \"cerebral_bleed\", \"hip_implant\", \"coronary_arteries\"\n",
    "EXTRA_MODELS = []  # Example: [\"body\", \"lung_vessels\"]\n",
    "\n",
    "# Force re-segmentation even if outputs exist\n",
    "FORCE_SEGMENTATION = False\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Custom nnUNet Models (Advanced)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "ENABLE_CUSTOM_MODELS = False  # Enable custom nnUNet models\n",
    "CUSTOM_MODELS_ROOT = \"/content/drive/MyDrive/custom_models\"  # Path to models\n",
    "CUSTOM_MODELS_SELECTED = []  # Example: [\"prostate_model\", \"brain_tumor\"]\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Custom Structures Configuration\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Custom structures YAML file (for combining/renaming ROIs)\n",
    "CUSTOM_STRUCTURES_FILE = \"custom_structures_pelvic.yaml\"  # Default for pelvic cases\n",
    "# Options: \"custom_structures_pelvic.yaml\", \"custom_structures_thorax.yaml\", or custom path\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Validation & Setup\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "LOGS_DIR = \"/content/logs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], check=True, capture_output=True, text=True)\n",
    "    gpu_available = True\n",
    "    # Extract GPU name\n",
    "    gpu_info = \"GPU detected\"\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'Tesla' in line or 'V100' in line or 'T4' in line or 'A100' in line:\n",
    "            gpu_info = line.strip().split()[2:4]\n",
    "            gpu_info = ' '.join(gpu_info)\n",
    "            break\n",
    "    print(f\"‚úÖ GPU: {gpu_info}\")\n",
    "except:\n",
    "    gpu_available = False\n",
    "    print(\"‚ö†Ô∏è No GPU detected!\")\n",
    "    print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "# Check DICOM directory\n",
    "if not os.path.exists(DICOM_ROOT):\n",
    "    print(f\"\\nüî¥ ERROR: DICOM directory not found!\")\n",
    "    print(f\"   Path: {DICOM_ROOT}\")\n",
    "    print(f\"\\n   Please update DICOM_ROOT in the cell above.\")\n",
    "else:\n",
    "    dicom_count = sum(1 for root, dirs, files in os.walk(DICOM_ROOT) \n",
    "                      for f in files if f.lower().endswith('.dcm'))\n",
    "    print(f\"\\n‚úÖ DICOM directory: {DICOM_ROOT}\")\n",
    "    print(f\"   {dicom_count} DICOM files\")\n",
    "\n",
    "print(f\"\\nüìã Configuration Summary:\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"   GPU: {'‚úÖ' if gpu_available else '‚ùå'}\")\n",
    "print(f\"   Course parallelism: {WORKERS} concurrent course(s)\")\n",
    "print(f\"   TotalSegmentator threads:\")\n",
    "print(f\"     ‚Ä¢ Resampling: {TOTALSEG_NR_THR_RESAMP}\")\n",
    "print(f\"     ‚Ä¢ Saving I/O: {TOTALSEG_NR_THR_SAVING}\")\n",
    "print(f\"     ‚Ä¢ Preprocessing: {TOTALSEG_NUM_PROC_PRE}\")\n",
    "print(f\"     ‚Ä¢ Export: {TOTALSEG_NUM_PROC_EXPORT}\")\n",
    "print(f\"   Fast mode: {'‚úÖ Enabled' if FAST_MODE else '‚ùå Disabled'}\")\n",
    "if ROI_SUBSET:\n",
    "    print(f\"   ROI subset: {ROI_SUBSET}\")\n",
    "if EXTRA_MODELS:\n",
    "    print(f\"   Extra models: {', '.join(EXTRA_MODELS)}\")\n",
    "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"\\nüì¶ Output: {DRIVE_OUTPUT_DIR}_YYYYMMDD_HHMMSS\")\n",
    "\n",
    "# Performance recommendations\n",
    "if WORKERS > 2 and gpu_available:\n",
    "    print(f\"\\n‚ö†Ô∏è PERFORMANCE NOTE: WORKERS={WORKERS} may cause memory issues in Colab\")\n",
    "    print(f\"   Recommended: WORKERS=1-2 for optimal GPU memory usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Generate Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build configuration\n",
    "config_yaml = f\"\"\"# RTpipeline Configuration - Part 1 (GPU Segmentation)\n",
    "dicom_root: \"{DICOM_ROOT}\"\n",
    "output_dir: \"{OUTPUT_DIR}\"\n",
    "logs_dir: \"{LOGS_DIR}\"\n",
    "workers: {WORKERS}\n",
    "\n",
    "segmentation:\n",
    "  workers: {SEG_WORKERS}\n",
    "  threads_per_worker: null\n",
    "  force: {str(FORCE_SEGMENTATION).lower()}\n",
    "  fast: {str(FAST_MODE).lower()}\n",
    "  roi_subset: {f'\"{ROI_SUBSET}\"' if ROI_SUBSET else 'null'}\n",
    "  extra_models: {EXTRA_MODELS if EXTRA_MODELS else '[]'}\n",
    "  device: \"{'gpu' if gpu_available else 'cpu'}\"\n",
    "  force_split: true\n",
    "  nr_threads_resample: {TOTALSEG_NR_THR_RESAMP}\n",
    "  nr_threads_save: {TOTALSEG_NR_THR_SAVING}\n",
    "  num_proc_preprocessing: {TOTALSEG_NUM_PROC_PRE}\n",
    "  num_proc_export: {TOTALSEG_NUM_PROC_EXPORT}\n",
    "\n",
    "custom_models:\n",
    "  enabled: {str(ENABLE_CUSTOM_MODELS).lower()}\n",
    "  root: \"{CUSTOM_MODELS_ROOT}\"\n",
    "  models: {CUSTOM_MODELS_SELECTED if CUSTOM_MODELS_SELECTED else '[]'}\n",
    "  workers: 1\n",
    "  force: false\n",
    "\n",
    "custom_structures: \"{CUSTOM_STRUCTURES_FILE}\"\n",
    "\"\"\"\n",
    "\n",
    "config_path = \"/content/config_part1.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(f\"‚úÖ Configuration written to: {config_path}\")\n",
    "print(f\"\\nYou can review the configuration:\")\n",
    "print(f\"   !cat {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Run Segmentation Pipeline\n",
    "\n",
    "This runs **ONLY** TotalSegmentator segmentation (GPU-accelerated)\n",
    "\n",
    "‚è±Ô∏è **Estimated Time:**\n",
    "- With GPU (T4): 10-20 minutes per patient\n",
    "- With GPU (V100/A100): 5-15 minutes per patient\n",
    "- Fast mode: ~3x faster\n",
    "- ROI subset: Proportionally faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import time\n",
    "\n",
    "os.environ['PATH'] = f\"/content/miniconda/bin:{os.environ.get('PATH', '')}\"\n",
    "os.chdir('/content/rtpipeline')\n",
    "\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"   RTpipeline Part 1: GPU Segmentation\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"\\n‚ö° Processing Mode:\")\n",
    "print(f\"   ‚Ä¢ GPU-accelerated segmentation\")\n",
    "print(f\"   ‚Ä¢ {WORKERS} concurrent course(s)\")\n",
    "print(f\"   ‚Ä¢ Fast mode: {'ON' if FAST_MODE else 'OFF'}\")\n",
    "print(f\"\\nDVH and radiomics will run in Part 2 (CPU)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Install Snakemake if needed\n",
    "try:\n",
    "    subprocess.run([\"conda\", \"run\", \"-n\", \"base\", \"snakemake\", \"--version\"],\n",
    "                   check=True, capture_output=True)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Installing Snakemake...\")\n",
    "    subprocess.run([\"conda\", \"install\", \"-n\", \"base\", \"-c\", \"conda-forge\", \n",
    "                    \"-c\", \"bioconda\", \"snakemake\", \"-y\", \"-q\"], check=True)\n",
    "    print(\"‚úÖ Snakemake installed\\n\")\n",
    "\n",
    "# Step 1: Organize courses\n",
    "print(\"[1/2] Organizing DICOM data...\")\n",
    "cmd_organize = [\n",
    "    \"conda\", \"run\", \"-n\", \"base\", \"snakemake\",\n",
    "    \"--configfile\", \"/content/config_part1.yaml\",\n",
    "    \"--use-conda\", \"--cores\", str(WORKERS),\n",
    "    \"--printshellcmds\",\n",
    "    \"/content/output/_COURSES/manifest.json\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd_organize, capture_output=False, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\n‚ö†Ô∏è Organization failed!\")\n",
    "else:\n",
    "    org_time = time.time()\n",
    "    print(f\"\\n‚úÖ Organization complete ({org_time - start_time:.1f}s)\\n\")\n",
    "    \n",
    "    # Step 2: Run segmentation\n",
    "    print(\"[2/2] Running TotalSegmentator...\")\n",
    "    \n",
    "    # Find all courses\n",
    "    seg_targets = []\n",
    "    custom_targets = []\n",
    "    \n",
    "    for patient_dir in glob.glob(f\"{OUTPUT_DIR}/*/\"):\n",
    "        patient_name = os.path.basename(patient_dir.rstrip('/'))\n",
    "        if patient_name.startswith('_') or patient_name.startswith('.'):\n",
    "            continue\n",
    "        for course_dir in glob.glob(f\"{patient_dir}/*/\"):\n",
    "            course_name = os.path.basename(course_dir.rstrip('/'))\n",
    "            if not course_name.startswith('_'):\n",
    "                seg_targets.append(f\"{OUTPUT_DIR}/{patient_name}/{course_name}/.segmentation_done\")\n",
    "                custom_targets.append(f\"{OUTPUT_DIR}/{patient_name}/{course_name}/.custom_models_done\")\n",
    "    \n",
    "    if seg_targets:\n",
    "        print(f\"Found {len(seg_targets)} course(s) to segment\")\n",
    "        print(f\"Estimated time: {len(seg_targets) * (5 if FAST_MODE else 15) / WORKERS:.0f}-{len(seg_targets) * (15 if FAST_MODE else 25) / WORKERS:.0f} minutes\\n\")\n",
    "        \n",
    "        # Run segmentation with resource limits for Colab\n",
    "        cmd_seg = [\n",
    "            \"conda\", \"run\", \"-n\", \"base\", \"snakemake\",\n",
    "            \"--configfile\", \"/content/config_part1.yaml\",\n",
    "            \"--use-conda\",\n",
    "            \"--cores\", str(WORKERS),\n",
    "            \"--resources\", f\"seg_workers={SEG_WORKERS}\",\n",
    "            \"--printshellcmds\",\n",
    "            \"--keep-going\"\n",
    "        ] + seg_targets + custom_targets\n",
    "        \n",
    "        result = subprocess.run(cmd_seg, capture_output=False, text=True)\n",
    "        \n",
    "        seg_time = time.time()\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\n‚úÖ All segmentations complete! ({seg_time - org_time:.1f}s)\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Some segmentations failed. Check logs.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No courses found\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Part 1 Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nOutputs: {OUTPUT_DIR}\")\n",
    "print(\"\\nNext: Run the cell below to save to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Outputs to Google Drive\n",
    "\n",
    "**IMPORTANT:** This saves your segmentation results to Google Drive for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "drive_output = f\"{DRIVE_OUTPUT_DIR}_{timestamp}\"\n",
    "\n",
    "print(f\"Copying outputs to Google Drive...\")\n",
    "print(f\"Destination: {drive_output}\\n\")\n",
    "\n",
    "try:\n",
    "    shutil.copytree(OUTPUT_DIR, drive_output)\n",
    "    shutil.copy(\"/content/config_part1.yaml\", f\"{drive_output}/config_part1.yaml\")\n",
    "    \n",
    "    # Create README for Part 2\n",
    "    readme = f\"\"\"RTpipeline Part 1 Outputs\n",
    "========================\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "To continue with Part 2:\n",
    "1. Open rtpipeline_colab_part2_cpu.ipynb\n",
    "2. In cell 5 (Configuration), set:\n",
    "   PART1_OUTPUT_DIR = \"{drive_output}\"\n",
    "3. Run all cells (on CPU runtime - no GPU needed!)\n",
    "\n",
    "Configuration used:\n",
    "- DICOM source: {DICOM_ROOT}\n",
    "- Workers: {WORKERS}\n",
    "- Fast mode: {FAST_MODE}\n",
    "- GPU: {gpu_available}\n",
    "\"\"\"\n",
    "    with open(f\"{drive_output}/README_PART2.txt\", 'w') as f:\n",
    "        f.write(readme)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ PART 1 COMPLETE - OUTPUTS SAVED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nSaved to: {drive_output}\")\n",
    "    print(\"\\nüìã Next Steps:\")\n",
    "    print(\"   1. You can disconnect this GPU runtime now\")\n",
    "    print(\"   2. Open rtpipeline_colab_part2_cpu.ipynb\")\n",
    "    print(f\"   3. Set PART1_OUTPUT_DIR = '{drive_output}'\")\n",
    "    print(\"   4. Run Part 2 on CPU runtime (saves GPU costs!)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error: {e}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - Google Drive is mounted\")\n",
    "    print(\"  - You have enough space\")\n",
    "    print(\"  - Path is valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Optional: View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"   Segmentation Summary\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\")\n",
    "\n",
    "total = 0\n",
    "completed = 0\n",
    "\n",
    "for patient_dir in sorted(glob.glob(f\"{OUTPUT_DIR}/*/\")):\n",
    "    patient_name = os.path.basename(patient_dir.rstrip('/'))\n",
    "    if patient_name.startswith('_') or patient_name.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    for course_dir in sorted(glob.glob(f\"{patient_dir}/*/\")):\n",
    "        course_name = os.path.basename(course_dir.rstrip('/'))\n",
    "        if course_name.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        seg_done = os.path.exists(f\"{course_dir}/.segmentation_done\")\n",
    "        \n",
    "        if seg_done:\n",
    "            completed += 1\n",
    "        \n",
    "        status = \"‚úÖ\" if seg_done else \"‚ö†Ô∏è\"\n",
    "        print(f\"{status} {patient_name}/{course_name}\")\n",
    "\n",
    "print(f\"\\nTotal: {completed}/{total} completed\")\n",
    "\n",
    "if completed == total and total > 0:\n",
    "    print(\"\\nüéâ All segmentations successful!\")\n",
    "elif completed > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è {total - completed} incomplete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No segmentations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**Continue with Part 2 (CPU):** `rtpipeline_colab_part2_cpu.ipynb`\n",
    "\n",
    "Part 2 will:\n",
    "- Extract DVH metrics\n",
    "- Compute radiomic features\n",
    "- Run robustness testing (optional)\n",
    "- Generate visualizations\n",
    "- Create downloadable results\n",
    "\n",
    "**üí∞ Cost Savings:** Part 2 runs on CPU only!\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 2.0 (Part 1 - GPU Segmentation)  \n",
    "**Repository:** https://github.com/kstawiski/rtpipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

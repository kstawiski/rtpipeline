{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTpipeline on Google Colab - Part 1: GPU Segmentation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kstawiski/rtpipeline/blob/main/rtpipeline_colab_part1_gpu.ipynb)\n",
    "\n",
    "**üí∞ Cost Optimization:** This notebook is split into two parts to optimize GPU costs:\n",
    "- **Part 1 (this notebook):** Runs TotalSegmentator with GPU (~10-30 min/patient)\n",
    "- **Part 2:** Runs DVH, radiomics, and analysis on CPU only (saves GPU costs)\n",
    "\n",
    "## What This Part Does\n",
    "\n",
    "‚úÖ **Automatic segmentation** of 100+ organs using TotalSegmentator (GPU-accelerated)\n",
    "‚úÖ **Saves outputs** to Google Drive for Part 2\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Colab with **GPU runtime** (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- DICOM files in Google Drive\n",
    "- Google Drive mounted for saving outputs\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö° Quick Start:** \n",
    "1. Run cells 1-3 (setup)\n",
    "2. Mount Google Drive (cell 4)\n",
    "3. **UPDATE CONFIGURATION** (cell 5) - Point to your DICOM folder\n",
    "4. Run remaining cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Install Miniconda & System Dependencies\n",
    "\n",
    "This takes ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check GPU availability\n",
    "echo \"=== GPU Check ===\"\n",
    "nvidia-smi || echo \"‚ö†Ô∏è No GPU detected. Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\"\n",
    "\n",
    "# Install system dependencies\n",
    "echo -e \"\\n=== Installing System Dependencies ===\"\n",
    "apt-get update -qq\n",
    "apt-get install -y -qq dcm2niix pigz > /dev/null\n",
    "\n",
    "# Install Miniconda if not already installed\n",
    "if [ ! -d \"/content/miniconda\" ]; then\n",
    "    echo -e \"\\n=== Installing Miniconda ===\"\n",
    "    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n",
    "    bash /tmp/miniconda.sh -b -p /content/miniconda\n",
    "    rm /tmp/miniconda.sh\n",
    "    echo \"‚úÖ Miniconda installed\"\n",
    "else\n",
    "    echo \"‚úÖ Miniconda already installed\"\n",
    "fi\n",
    "\n",
    "# Initialize conda\n",
    "export PATH=\"/content/miniconda/bin:$PATH\"\n",
    "eval \"$(/content/miniconda/bin/conda shell.bash hook)\"\n",
    "conda init bash\n",
    "\n",
    "echo -e \"\\n‚úÖ Setup complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone RTpipeline Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"/content/rtpipeline\" ]; then\n",
    "    echo \"Cloning rtpipeline repository...\"\n",
    "    git clone -q https://github.com/kstawiski/rtpipeline.git /content/rtpipeline\n",
    "    echo \"‚úÖ Repository cloned\"\n",
    "else\n",
    "    echo \"‚úÖ Repository already exists\"\n",
    "    cd /content/rtpipeline\n",
    "    git pull origin main\n",
    "    echo \"Repository updated\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Conda Environment\n",
    "\n",
    "This creates the rtpipeline environment for TotalSegmentator (~5-10 minutes, only once per session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"/content/miniconda/bin:$PATH\"\n",
    "eval \"$(/content/miniconda/bin/conda shell.bash hook)\"\n",
    "\n",
    "# Accept Anaconda Terms of Service\n",
    "echo \"=== Accepting Anaconda Terms of Service ===\"\n",
    "conda config --set channel_priority flexible\n",
    "if ! conda tos accept --channel defaults 2>&1; then\n",
    "    echo \"‚ö†Ô∏è ToS acceptance failed or already accepted\"\n",
    "fi\n",
    "echo \"‚úÖ ToS accepted\"\n",
    "\n",
    "cd /content/rtpipeline\n",
    "\n",
    "# Create rtpipeline environment\n",
    "if conda env list | grep -q \"^rtpipeline \"; then\n",
    "    echo \"‚úÖ Environment 'rtpipeline' already exists\"\n",
    "else\n",
    "    echo \"Creating 'rtpipeline' environment (TotalSegmentator)...\"\n",
    "    conda env create -f envs/rtpipeline.yaml -q\n",
    "    echo \"‚úÖ Environment created\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "conda run -n rtpipeline python -c \"import numpy; print(f'‚úÖ numpy {numpy.__version__}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Mount Google Drive\n",
    "\n",
    "**IMPORTANT:** Your DICOM files must be in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted at /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURATION - UPDATE THIS!\n",
    "\n",
    "## 5Ô∏è‚É£ Configure Input/Output Paths\n",
    "\n",
    "**üî¥ REQUIRED:** Update `DICOM_ROOT` to point to your DICOM files in Google Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üî¥ UPDATE THIS - Point to your DICOM folder in Google Drive\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "DICOM_ROOT = \"/content/drive/MyDrive/my_dicom_folder\"\n",
    "\n",
    "# Examples:\n",
    "# DICOM_ROOT = \"/content/drive/MyDrive/RT_Data/DICOM\"\n",
    "# DICOM_ROOT = \"/content/drive/MyDrive/Patient_Data\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Output location in Google Drive (for Part 2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/rtpipeline_part1_output\"\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Optional: Processing settings (usually no need to change)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "WORKERS = 2              # I/O parallelism (2-4 recommended)\n",
    "SEG_WORKERS = 1          # GPU workers (keep at 1 for single GPU)\n",
    "FAST_MODE = False        # True = faster but lower quality\n",
    "ENABLE_CUSTOM_MODELS = False  # Enable custom nnUNet models\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Validation\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Local working directories\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "LOGS_DIR = \"/content/logs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    subprocess.run(['nvidia-smi'], check=True, capture_output=True)\n",
    "    gpu_available = True\n",
    "    print(\"‚úÖ GPU detected\")\n",
    "except:\n",
    "    gpu_available = False\n",
    "    print(\"‚ö†Ô∏è No GPU detected!\")\n",
    "    print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "# Check DICOM directory\n",
    "if not os.path.exists(DICOM_ROOT):\n",
    "    print(f\"\\nüî¥ ERROR: DICOM directory not found!\")\n",
    "    print(f\"   Path: {DICOM_ROOT}\")\n",
    "    print(f\"\\n   Please update DICOM_ROOT in the cell above.\")\n",
    "    print(f\"   Your DICOM files must be in Google Drive.\")\n",
    "else:\n",
    "    dicom_count = sum(1 for root, dirs, files in os.walk(DICOM_ROOT) \n",
    "                      for f in files if f.endswith('.dcm'))\n",
    "    print(f\"\\n‚úÖ DICOM directory found: {DICOM_ROOT}\")\n",
    "    print(f\"   {dicom_count} DICOM files detected\")\n",
    "\n",
    "print(f\"\\nüìã Configuration Summary:\")\n",
    "print(f\"   GPU: {'‚úÖ Available' if gpu_available else '‚ùå Not available'}\")\n",
    "print(f\"   Workers: {WORKERS}\")\n",
    "print(f\"   Fast Mode: {'‚úÖ Enabled' if FAST_MODE else '‚ùå Disabled'}\")\n",
    "print(f\"\\nüì¶ Outputs will be saved to: {DRIVE_OUTPUT_DIR}_YYYYMMDD_HHMMSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Generate Configuration File\n",
    "\n",
    "This creates the pipeline configuration based on your settings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_yaml = f\"\"\"# RTpipeline Configuration - Part 1 (GPU Segmentation)\n",
    "dicom_root: \"{DICOM_ROOT}\"\n",
    "output_dir: \"{OUTPUT_DIR}\"\n",
    "logs_dir: \"{LOGS_DIR}\"\n",
    "workers: {WORKERS}\n",
    "\n",
    "segmentation:\n",
    "  workers: {SEG_WORKERS}\n",
    "  fast: {str(FAST_MODE).lower()}\n",
    "  device: \"{'gpu' if gpu_available else 'cpu'}\"\n",
    "  force_split: true\n",
    "  nr_threads_resample: 1\n",
    "  nr_threads_save: 1\n",
    "  num_proc_preprocessing: 1\n",
    "  num_proc_export: 1\n",
    "\n",
    "custom_models:\n",
    "  enabled: {str(ENABLE_CUSTOM_MODELS).lower()}\n",
    "\n",
    "custom_structures: \"custom_structures_pelvic.yaml\"\n",
    "\"\"\"\n",
    "\n",
    "config_path = \"/content/config_part1.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(f\"‚úÖ Configuration written to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Run Segmentation Pipeline\n",
    "\n",
    "This runs **ONLY** TotalSegmentator segmentation (GPU-accelerated)\n",
    "\n",
    "‚è±Ô∏è **Time:** 10-30 minutes per patient with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "os.environ['PATH'] = f\"/content/miniconda/bin:{os.environ.get('PATH', '')}\"\n",
    "os.chdir('/content/rtpipeline')\n",
    "\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"   RTpipeline Part 1: GPU Segmentation\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"\\nThis will run ONLY segmentation (GPU-accelerated)\")\n",
    "print(\"DVH and radiomics will run in Part 2 (CPU)\\n\")\n",
    "\n",
    "# Install Snakemake if needed\n",
    "try:\n",
    "    subprocess.run([\"conda\", \"run\", \"-n\", \"base\", \"snakemake\", \"--version\"],\n",
    "                   check=True, capture_output=True)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Installing Snakemake...\")\n",
    "    subprocess.run([\"conda\", \"install\", \"-n\", \"base\", \"-c\", \"conda-forge\", \n",
    "                    \"-c\", \"bioconda\", \"snakemake\", \"-y\", \"-q\"], check=True)\n",
    "    print(\"‚úÖ Snakemake installed\\n\")\n",
    "\n",
    "# Step 1: Organize courses\n",
    "print(\"[1/2] Organizing DICOM data...\")\n",
    "cmd_organize = [\n",
    "    \"conda\", \"run\", \"-n\", \"base\", \"snakemake\",\n",
    "    \"--configfile\", \"/content/config_part1.yaml\",\n",
    "    \"--use-conda\", \"--cores\", str(WORKERS),\n",
    "    \"--printshellcmds\",\n",
    "    \"/content/output/_COURSES/manifest.json\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd_organize, capture_output=False, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\n‚ö†Ô∏è Organization failed!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Organization complete\\n\")\n",
    "    \n",
    "    # Step 2: Run segmentation\n",
    "    print(\"[2/2] Running TotalSegmentator...\")\n",
    "    \n",
    "    # Find all courses\n",
    "    seg_targets = []\n",
    "    custom_targets = []\n",
    "    \n",
    "    for patient_dir in glob.glob(f\"{OUTPUT_DIR}/*/\"):\n",
    "        patient_name = os.path.basename(patient_dir.rstrip('/'))\n",
    "        if patient_name.startswith('_') or patient_name.startswith('.'):\n",
    "            continue\n",
    "        for course_dir in glob.glob(f\"{patient_dir}/*/\"):\n",
    "            course_name = os.path.basename(course_dir.rstrip('/'))\n",
    "            if not course_name.startswith('_'):\n",
    "                seg_targets.append(f\"{OUTPUT_DIR}/{patient_name}/{course_name}/.segmentation_done\")\n",
    "                custom_targets.append(f\"{OUTPUT_DIR}/{patient_name}/{course_name}/.custom_models_done\")\n",
    "    \n",
    "    if seg_targets:\n",
    "        print(f\"Found {len(seg_targets)} course(s) to segment\\n\")\n",
    "        \n",
    "        cmd_seg = [\n",
    "            \"conda\", \"run\", \"-n\", \"base\", \"snakemake\",\n",
    "            \"--configfile\", \"/content/config_part1.yaml\",\n",
    "            \"--use-conda\", \"--cores\", str(WORKERS),\n",
    "            \"--printshellcmds\", \"--keep-going\"\n",
    "        ] + seg_targets + custom_targets\n",
    "        \n",
    "        result = subprocess.run(cmd_seg, capture_output=False, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n‚úÖ All segmentations complete!\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Some segmentations failed. Check logs.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No courses found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Part 1 Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOutputs: {OUTPUT_DIR}\")\n",
    "print(\"\\nNext: Run the cell below to save to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Outputs to Google Drive\n",
    "\n",
    "**IMPORTANT:** This saves your segmentation results to Google Drive for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "drive_output = f\"{DRIVE_OUTPUT_DIR}_{timestamp}\"\n",
    "\n",
    "print(f\"Copying outputs to Google Drive...\")\n",
    "print(f\"Destination: {drive_output}\\n\")\n",
    "\n",
    "try:\n",
    "    shutil.copytree(OUTPUT_DIR, drive_output)\n",
    "    shutil.copy(\"/content/config_part1.yaml\", f\"{drive_output}/config_part1.yaml\")\n",
    "    \n",
    "    # Create README for Part 2\n",
    "    readme = f\"\"\"RTpipeline Part 1 Outputs\n",
    "========================\n",
    "Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "To continue with Part 2:\n",
    "1. Open rtpipeline_colab_part2_cpu.ipynb\n",
    "2. In cell 5 (Configuration), set:\n",
    "   PART1_OUTPUT_DIR = \"{drive_output}\"\n",
    "3. Run all cells (on CPU runtime - no GPU needed!)\n",
    "\n",
    "DICOM source: {DICOM_ROOT}\n",
    "\"\"\"\n",
    "    with open(f\"{drive_output}/README_PART2.txt\", 'w') as f:\n",
    "        f.write(readme)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ PART 1 COMPLETE - OUTPUTS SAVED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nSaved to: {drive_output}\")\n",
    "    print(\"\\nüìã Next Steps:\")\n",
    "    print(\"   1. You can disconnect this GPU runtime now\")\n",
    "    print(\"   2. Open rtpipeline_colab_part2_cpu.ipynb\")\n",
    "    print(f\"   3. Set PART1_OUTPUT_DIR = '{drive_output}'\")\n",
    "    print(\"   4. Run Part 2 on CPU runtime (saves GPU costs!)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error: {e}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  - Google Drive is mounted\")\n",
    "    print(\"  - You have enough space\")\n",
    "    print(\"  - Path is valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Optional: View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"   Segmentation Summary\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\")\n",
    "\n",
    "total = 0\n",
    "completed = 0\n",
    "\n",
    "for patient_dir in sorted(glob.glob(f\"{OUTPUT_DIR}/*/\")):\n",
    "    patient_name = os.path.basename(patient_dir.rstrip('/'))\n",
    "    if patient_name.startswith('_') or patient_name.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    for course_dir in sorted(glob.glob(f\"{patient_dir}/*/\")):\n",
    "        course_name = os.path.basename(course_dir.rstrip('/'))\n",
    "        if course_name.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        seg_done = os.path.exists(f\"{course_dir}/.segmentation_done\")\n",
    "        \n",
    "        if seg_done:\n",
    "            completed += 1\n",
    "        \n",
    "        status = \"‚úÖ\" if seg_done else \"‚ö†Ô∏è\"\n",
    "        print(f\"{status} {patient_name}/{course_name}\")\n",
    "\n",
    "print(f\"\\nTotal: {completed}/{total} completed\")\n",
    "\n",
    "if completed == total and total > 0:\n",
    "    print(\"\\nüéâ All segmentations successful!\")\n",
    "elif completed > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è {total - completed} incomplete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No segmentations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**Continue with Part 2 (CPU):** `rtpipeline_colab_part2_cpu.ipynb`\n",
    "\n",
    "Part 2 will:\n",
    "- Extract DVH metrics\n",
    "- Compute radiomic features\n",
    "- Run robustness testing (optional)\n",
    "- Generate visualizations\n",
    "- Create downloadable results\n",
    "\n",
    "**üí∞ Cost Savings:** Part 2 runs on CPU only!\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 2.0 (Part 1 - GPU Segmentation)  \n",
    "**Repository:** https://github.com/kstawiski/rtpipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

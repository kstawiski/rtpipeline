{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTpipeline on Google Colab - Part 1: GPU Segmentation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kstawiski/rtpipeline/blob/main/rtpipeline_colab_part1_gpu.ipynb)\n",
    "\n",
    "**üí∞ Cost Optimization:** This notebook is split into two parts to optimize GPU costs:\n",
    "- **Part 1 (this notebook):** Runs TotalSegmentator with GPU (~10-30 min/patient)\n",
    "- **Part 2:** Runs DVH, radiomics, and analysis on CPU only (saves GPU costs)\n",
    "\n",
    "## What This Part Does\n",
    "\n",
    "‚úÖ **Automatic segmentation** of 100+ organs using TotalSegmentator (GPU-accelerated)\n",
    "‚úÖ **Saves outputs** to Google Drive for Part 2\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Colab with **GPU runtime** (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- DICOM files in Google Drive\n",
    "- Google Drive mounted for saving outputs\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö° Quick Start:** \n",
    "1. Run cells 1-3 (setup)\n",
    "2. Mount Google Drive (cell 4)\n",
    "3. **UPDATE CONFIGURATION** (cell 5) - Point to your DICOM folder\n",
    "4. Run remaining cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Install Miniconda & System Dependencies\n",
    "\n",
    "This takes ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU Check ===\n",
      "‚ö†Ô∏è No GPU detected. Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
      "\n",
      "=== Installing System Dependencies ===\n",
      "\n",
      "=== Installing Python dependencies (pydicom, SimpleITK, etc.) ===\n",
      "‚úÖ Core Python deps installed\n",
      "\n",
      "=== Installing Miniconda ===\n",
      "PREFIX=/content/miniconda\n",
      "Unpacking bootstrapper...\n",
      "Unpacking payload...\n",
      "\n",
      "Installing base environment...\n",
      "\n",
      "Preparing transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "installation finished.\n",
      "WARNING:\n",
      "    You currently have a PYTHONPATH environment variable set. This may cause\n",
      "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
      "    For best results, please verify that your PYTHONPATH only points to\n",
      "    directories of packages that are compatible with the Python interpreter\n",
      "    in Miniconda3: /content/miniconda\n",
      "‚úÖ Miniconda installed\n",
      "no change     /content/miniconda/condabin/conda\n",
      "no change     /content/miniconda/bin/conda\n",
      "no change     /content/miniconda/bin/conda-env\n",
      "no change     /content/miniconda/bin/activate\n",
      "no change     /content/miniconda/bin/deactivate\n",
      "no change     /content/miniconda/etc/profile.d/conda.sh\n",
      "no change     /content/miniconda/etc/fish/conf.d/conda.fish\n",
      "no change     /content/miniconda/shell/condabin/Conda.psm1\n",
      "no change     /content/miniconda/shell/condabin/conda-hook.ps1\n",
      "no change     /content/miniconda/lib/python3.13/site-packages/xontrib/conda.xsh\n",
      "no change     /content/miniconda/etc/profile.d/conda.csh\n",
      "modified      /root/.bashrc\n",
      "\n",
      "==> For changes to take effect, close and re-open your current shell. <==\n",
      "\n",
      "\n",
      "=== Installing Snakemake (base env) ===\n",
      "Jupyter detected...\n",
      "\n",
      "‚úÖ Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 3: nvidia-smi: command not found\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "ERROR: Could not find a version that satisfies the requirement rt-utils>=1.4.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.5, 1.2.6, 1.2.7)\n",
      "ERROR: No matching distribution found for rt-utils>=1.4.0\n",
      "\n",
      "CondaToSNonInteractiveError: Terms of Service have not been accepted for the following channels. Please accept or remove them before proceeding:\n",
      "    - https://repo.anaconda.com/pkgs/main\n",
      "    - https://repo.anaconda.com/pkgs/r\n",
      "\n",
      "To accept these channels' Terms of Service, run the following commands:\n",
      "    conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/main\n",
      "    conda tos accept --override-channels --channel https://repo.anaconda.com/pkgs/r\n",
      "\n",
      "For information on safely removing channels from your conda configuration,\n",
      "please see the official documentation:\n",
      "\n",
      "    https://www.anaconda.com/docs/tools/working-with-conda/channels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check GPU availability\n",
    "echo \"=== GPU Check ===\"\n",
    "nvidia-smi || echo \"‚ö†Ô∏è No GPU detected. Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\"\n",
    "\n",
    "# Install system dependencies\n",
    "echo -e \"\\n=== Installing System Dependencies ===\"\n",
    "apt-get update -qq\n",
    "apt-get install -y -qq dcm2niix pigz > /dev/null\n",
    "\n",
    "echo -e \"\\n=== Installing Python dependencies (pydicom, SimpleITK, etc.) ===\"\n",
    "python3 -m pip install -q \"pydicom>=3.0.0\" \"SimpleITK>=2.3.0\" \"dicompyler-core>=0.5.6\" \"rt-utils>=1.4.0\" \"nibabel>=5.1.0\" \"xlsxwriter\" \"openpyxl\"\n",
    "echo \"‚úÖ Core Python deps installed\"\n",
    "\n",
    "# Install Miniconda if not already installed\n",
    "if [ ! -d \"/content/miniconda\" ]; then\n",
    "    echo -e \"\\n=== Installing Miniconda ===\"\n",
    "    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n",
    "    bash /tmp/miniconda.sh -b -p /content/miniconda\n",
    "    rm /tmp/miniconda.sh\n",
    "    echo \"‚úÖ Miniconda installed\"\n",
    "else\n",
    "    echo \"‚úÖ Miniconda already installed\"\n",
    "fi\n",
    "\n",
    "# Initialize conda\n",
    "export PATH=\"/content/miniconda/bin:$PATH\"\n",
    "eval \"$(/content/miniconda/bin/conda shell.bash hook)\"\n",
    "conda init bash\n",
    "\n",
    "\n",
    "echo -e \"\\n=== Installing Snakemake (base env) ===\"\n",
    "conda install -n base -c conda-forge -c bioconda -y -q snakemake\n",
    "echo -e \"\\n‚úÖ Setup complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone RTpipeline Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning rtpipeline repository...\n",
      "‚úÖ Repository cloned\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d \"/content/rtpipeline\" ]; then\n",
    "    echo \"Cloning rtpipeline repository...\"\n",
    "    git clone -q https://github.com/kstawiski/rtpipeline.git /content/rtpipeline\n",
    "    echo \"‚úÖ Repository cloned\"\n",
    "else\n",
    "    echo \"‚úÖ Repository already exists\"\n",
    "    cd /content/rtpipeline\n",
    "    git pull origin main\n",
    "    echo \"Repository updated\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Conda Environment\n",
    "\n",
    "This creates the rtpipeline environment for TotalSegmentator (~5-10 minutes, only once per session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=\"/content/miniconda/bin:$PATH\"\n",
    "eval \"$(/content/miniconda/bin/conda shell.bash hook)\"\n",
    "\n",
    "# Accept Anaconda Terms of Service\n",
    "echo \"=== Accepting Anaconda Terms of Service ===\"\n",
    "conda config --set channel_priority flexible\n",
    "if ! conda tos accept --channel defaults 2>&1; then\n",
    "    echo \"‚ö†Ô∏è ToS acceptance failed or already accepted\"\n",
    "fi\n",
    "echo \"‚úÖ ToS accepted\"\n",
    "\n",
    "cd /content/rtpipeline\n",
    "\n",
    "# Create rtpipeline environment\n",
    "if conda env list | grep -q \"^rtpipeline \"; then\n",
    "    echo \"‚úÖ Environment 'rtpipeline' already exists\"\n",
    "else\n",
    "    echo \"Creating 'rtpipeline' environment (TotalSegmentator)...\"\n",
    "    conda env create -f envs/rtpipeline.yaml -q\n",
    "    echo \"‚úÖ Environment created\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "conda run -n rtpipeline python -c \"import numpy; print(f'‚úÖ numpy {numpy.__version__}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Mount Google Drive\n",
    "\n",
    "**IMPORTANT:** Your DICOM files must be in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3274149031.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ Google Drive mounted at /content/drive/MyDrive/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted at /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è CONFIGURATION - UPDATE THIS!\n",
    "\n",
    "## 5Ô∏è‚É£ Configure Input/Output Paths & Processing Options\n",
    "\n",
    "**üî¥ REQUIRED:** Update `DICOM_ROOT` to point to your DICOM files in Google Drive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "DICOM_ROOT = \"/content/drive/MyDrive/my_dicom_folder\"\n",
    "\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_DIR = f\"/content/drive/MyDrive/rtpipeline_part1_output_{RUN_TIMESTAMP}\"\n",
    "LOGS_DIR = \"/content/logs\"\n",
    "LOCAL_TEMP_DIR = \"/content/tmp_part1\"\n",
    "SEG_TEMP_DIR = os.path.join(LOCAL_TEMP_DIR, \"totalseg\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "os.makedirs(SEG_TEMP_DIR, exist_ok=True)\n",
    "\n",
    "CPU_COUNT = os.cpu_count() or 2\n",
    "WORKERS = max(1, CPU_COUNT - 1)\n",
    "SNAKEMAKE_JOB_THREADS = WORKERS\n",
    "SEG_WORKERS = 1\n",
    "\n",
    "TOTALSEG_NR_THR_RESAMP = 1\n",
    "TOTALSEG_NR_THR_SAVING = 6\n",
    "TOTALSEG_NUM_PROC_PRE = 6\n",
    "TOTALSEG_NUM_PROC_EXPORT = 6\n",
    "\n",
    "FAST_MODE = False\n",
    "ROI_SUBSET = None\n",
    "EXTRA_MODELS = []\n",
    "FORCE_SEGMENTATION = False\n",
    "\n",
    "ENABLE_CUSTOM_MODELS = False\n",
    "CUSTOM_MODELS_ROOT = \"/content/drive/MyDrive/custom_models\"\n",
    "CUSTOM_MODELS_SELECTED = []\n",
    "\n",
    "CUSTOM_STRUCTURES_FILE = \"custom_structures_pelvic.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Generate Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import yaml\n",
    "except ImportError:\n",
    "    import subprocess as _subprocess\n",
    "    import sys as _sys\n",
    "    _subprocess.check_call([_sys.executable, '-m', 'pip', 'install', 'pyyaml'])\n",
    "    import yaml\n",
    "\n",
    "config_data = {\n",
    "    'dicom_root': DICOM_ROOT,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'logs_dir': LOGS_DIR,\n",
    "    'snakemake_job_threads': WORKERS,\n",
    "    'workers': WORKERS,\n",
    "    'segmentation': {\n",
    "        'workers': SEG_WORKERS,\n",
    "        'threads_per_worker': None,\n",
    "        'force': bool(FORCE_SEGMENTATION),\n",
    "        'fast': bool(FAST_MODE),\n",
    "        'roi_subset': ROI_SUBSET if ROI_SUBSET else None,\n",
    "        'extra_models': EXTRA_MODELS or [],\n",
    "        'device': 'gpu' if gpu_available else 'cpu',\n",
    "        'force_split': True,\n",
    "        'nr_threads_resample': TOTALSEG_NR_THR_RESAMP,\n",
    "        'nr_threads_save': TOTALSEG_NR_THR_SAVING,\n",
    "        'num_proc_preprocessing': TOTALSEG_NUM_PROC_PRE,\n",
    "        'num_proc_export': TOTALSEG_NUM_PROC_EXPORT\n",
    "    },\n",
    "    'custom_models': {\n",
    "        'enabled': bool(ENABLE_CUSTOM_MODELS),\n",
    "        'root': CUSTOM_MODELS_ROOT,\n",
    "        'models': CUSTOM_MODELS_SELECTED or [],\n",
    "        'workers': 1,\n",
    "        'force': False\n",
    "    },\n",
    "    'custom_structures': CUSTOM_STRUCTURES_FILE\n",
    "}\n",
    "\n",
    "config_path = '/content/config_part1.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write('# RTpipeline Configuration - Part 1 (GPU Segmentation)\\n')\n",
    "    yaml.safe_dump(config_data, f, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ Configuration written to: {config_path}\")\n",
    "print(f\"\\nYou can review the configuration:\")\n",
    "print(f\"   !cat {config_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Run Segmentation Pipeline\n",
    "\n",
    "This runs **ONLY** TotalSegmentator segmentation (GPU-accelerated)\n",
    "\n",
    "‚è±Ô∏è **Estimated Time:**\n",
    "- With GPU (T4): 10-20 minutes per patient\n",
    "- With GPU (V100/A100): 5-15 minutes per patient\n",
    "- Fast mode: ~3x faster\n",
    "- ROI subset: Proportionally faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import time\n",
    "\n",
    "os.environ['PATH'] = f\"/content/miniconda/bin:{os.environ.get('PATH', '')}\"\n",
    "os.chdir('/content/rtpipeline')\n",
    "\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"   RTpipeline Part 1: GPU Segmentation\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"\\n‚ö° Processing Mode:\")\n",
    "print(f\"   ‚Ä¢ GPU-accelerated segmentation\")\n",
    "print(f\"   ‚Ä¢ {WORKERS} concurrent course(s)\")\n",
    "print(f\"   ‚Ä¢ Fast mode: {'ON' if FAST_MODE else 'OFF'}\")\n",
    "print(f\"\\nDVH and radiomics will run in Part 2 (CPU)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Install Snakemake if needed\n",
    "try:\n",
    "    subprocess.run([\"conda\", \"run\", \"-n\", \"base\", \"snakemake\", \"--version\"],\n",
    "                   check=True, capture_output=True)\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Installing Snakemake...\")\n",
    "    subprocess.run([\"conda\", \"install\", \"-n\", \"base\", \"-c\", \"conda-forge\", \n",
    "                    \"-c\", \"bioconda\", \"snakemake\", \"-y\", \"-q\"], check=True)\n",
    "    print(\"‚úÖ Snakemake installed\\n\")\n",
    "\n",
    "# Step 1: Organize courses\n",
    "print(\"[1/2] Organizing DICOM data...\")\n",
    "cmd_organize = [\n",
    "    \"conda\", \"run\", \"-n\", \"base\", \"snakemake\",\n",
    "    \"--configfile\", \"/content/config_part1.yaml\",\n",
    "    \"--use-conda\", \"--cores\", str(WORKERS),\n",
    "    \"--printshellcmds\",\n",
    "    \"/content/output/_COURSES/manifest.json\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd_organize, capture_output=False, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\n‚ö†Ô∏è Organization failed!\")\n",
    "else:\n",
    "    org_time = time.time()\n",
    "    print(f\"\\n‚úÖ Organization complete ({org_time - start_time:.1f}s)\\n\")\n",
    "    \n",
    "    # Step 2: Run segmentation\n",
    "    print(\"[2/2] Running TotalSegmentator...\")\n",
    "    \n",
    "    # Find all courses\n",
    "    seg_targets = []\n",
    "    custom_targets = []\n",
    "    \n",
    "    for patient_dir in glob.glob(f\"{OUTPUT_DIR}/*/\"):\n",
    "        patient_name = os.path.basename(patient_dir.rstrip('/'))\n",
    "        if patient_name.startswith('_') or patient_name.startswith('.'):\n",
    "            continue\n",
    "        for course_dir in glob.glob(f\"{patient_dir}/*/\"):\n",
    "            course_name = os.path.basename(course_dir.rstrip('/'))\n",
    "            if not course_name.startswith('_'):\n",
    "                seg_targets.append(f\"{OUTPUT_DIR}/{patient_name}/{course_name}/.segmentation_done\")\n",
    "                custom_targets.append(f\"{OUTPUT_DIR}/{patient_name}/{course_name}/.custom_models_done\")\n",
    "    \n",
    "    if seg_targets:\n",
    "        print(f\"Found {len(seg_targets)} course(s) to segment\")\n",
    "        print(f\"Estimated time: {len(seg_targets) * (5 if FAST_MODE else 15) / WORKERS:.0f}-{len(seg_targets) * (15 if FAST_MODE else 25) / WORKERS:.0f} minutes\\n\")\n",
    "        \n",
    "        # Run segmentation with resource limits for Colab\n",
    "        cmd_seg = [\n",
    "            \"conda\", \"run\", \"-n\", \"base\", \"snakemake\",\n",
    "            \"--configfile\", \"/content/config_part1.yaml\",\n",
    "            \"--use-conda\",\n",
    "            \"--cores\", str(WORKERS),\n",
    "            \"--resources\", f\"seg_workers={SEG_WORKERS}\",\n",
    "            \"--printshellcmds\",\n",
    "            \"--keep-going\"\n",
    "        ] + seg_targets + custom_targets\n",
    "        \n",
    "        result = subprocess.run(cmd_seg, capture_output=False, text=True)\n",
    "        \n",
    "        seg_time = time.time()\n",
    "        if result.returncode == 0:\n",
    "            print(f\"\\n‚úÖ All segmentations complete! ({seg_time - org_time:.1f}s)\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Some segmentations failed. Check logs.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No courses found\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Part 1 Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"\\nOutputs: {OUTPUT_DIR}\")\n",
    "print(\"\\nNext: Run the cell below to save to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Outputs to Google Drive\n",
    "\n",
    "**IMPORTANT:** This saves your segmentation results to Google Drive for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def _gpu_present():\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.run(['nvidia-smi'], check=True, capture_output=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "readme_path = os.path.join(OUTPUT_DIR, \"README_PART2.txt\")\n",
    "os.makedirs(os.path.dirname(readme_path), exist_ok=True)\n",
    "readme_template = \"\"\"RTpipeline Part 1 Outputs\n",
    "========================\n",
    "Generated: {timestamp}\n",
    "\n",
    "To continue with Part 2:\n",
    "1. Open rtpipeline_colab_part2_cpu.ipynb\n",
    "2. Set PART1_OUTPUT_DIR = \"{output}\"\n",
    "3. Run all cells (CPU runtime is sufficient)\n",
    "\n",
    "Configuration summary:\n",
    "- DICOM source: {dicom}\n",
    "- Workers: {workers}\n",
    "- Fast mode: {fast}\n",
    "- GPU detected: {gpu}\n",
    "\"\"\"\n",
    "readme_text = readme_template.format(\n",
    "    timestamp=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    output=OUTPUT_DIR,\n",
    "    dicom=DICOM_ROOT,\n",
    "    workers=WORKERS,\n",
    "    fast=FAST_MODE,\n",
    "    gpu='Yes' if _gpu_present() else 'No'\n",
    ")\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_text)\n",
    "\n",
    "print(\"\n",
    "\" + \"=\"*60)\n",
    "print(\"üéâ PART 1 COMPLETE - OUTPUTS SAVED DIRECTLY TO GOOGLE DRIVE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\n",
    "Outputs stored at: {OUTPUT_DIR}\")\n",
    "print(\"\n",
    "üìã Next Steps:\")\n",
    "print(\"   1. You can disconnect this GPU runtime now\")\n",
    "print(\"   2. Open rtpipeline_colab_part2_cpu.ipynb\")\n",
    "print(f\"   3. Set PART1_OUTPUT_DIR = '{OUTPUT_DIR}'\")\n",
    "print(\"   4. Run Part 2 on CPU runtime (no GPU needed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Optional: View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(\"   Segmentation Summary\")\n",
    "print(\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\\n\")\n",
    "\n",
    "total = 0\n",
    "completed = 0\n",
    "\n",
    "for patient_dir in sorted(glob.glob(f\"{OUTPUT_DIR}/*/\")):\n",
    "    patient_name = os.path.basename(patient_dir.rstrip('/'))\n",
    "    if patient_name.startswith('_') or patient_name.startswith('.'):\n",
    "        continue\n",
    "    \n",
    "    for course_dir in sorted(glob.glob(f\"{patient_dir}/*/\")):\n",
    "        course_name = os.path.basename(course_dir.rstrip('/'))\n",
    "        if course_name.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        total += 1\n",
    "        seg_done = os.path.exists(f\"{course_dir}/.segmentation_done\")\n",
    "        \n",
    "        if seg_done:\n",
    "            completed += 1\n",
    "        \n",
    "        status = \"‚úÖ\" if seg_done else \"‚ö†Ô∏è\"\n",
    "        print(f\"{status} {patient_name}/{course_name}\")\n",
    "\n",
    "print(f\"\\nTotal: {completed}/{total} completed\")\n",
    "\n",
    "if completed == total and total > 0:\n",
    "    print(\"\\nüéâ All segmentations successful!\")\n",
    "elif completed > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è {total - completed} incomplete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No segmentations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "**Continue with Part 2 (CPU):** `rtpipeline_colab_part2_cpu.ipynb`\n",
    "\n",
    "Part 2 will:\n",
    "- Extract DVH metrics\n",
    "- Compute radiomic features\n",
    "- Run robustness testing (optional)\n",
    "- Generate visualizations\n",
    "- Create downloadable results\n",
    "\n",
    "**üí∞ Cost Savings:** Part 2 runs on CPU only!\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 2.0 (Part 1 - GPU Segmentation)  \n",
    "**Repository:** https://github.com/kstawiski/rtpipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

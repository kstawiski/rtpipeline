{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# RTpipeline on Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kstawiski/rtpipeline/blob/main/rtpipeline_colab.ipynb)\n",
    "\n",
    "This notebook allows you to run the RTpipeline radiotherapy data processing system on Google Colab with GPU acceleration.\n",
    "\n",
    "## What This Pipeline Does\n",
    "\n",
    "RTpipeline processes DICOM radiotherapy data and generates:\n",
    "- ‚úÖ **Automatic segmentation** of 100+ organs using TotalSegmentator\n",
    "- ‚úÖ **DVH metrics** (dose-volume histograms)\n",
    "- ‚úÖ **Radiomics features** (150+ texture/shape features)\n",
    "- ‚úÖ **Quality control reports**\n",
    "- ‚úÖ **Analysis-ready tables** for machine learning\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Colab account (free tier works, but GPU runtime recommended)\n",
    "- DICOM files (CT, RTPLAN, RTDOSE, RTSTRUCT)\n",
    "- ~10-30 minutes processing time per patient (GPU)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö° Quick Start:** Run all cells in order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1Ô∏è‚É£ Setup: Install Dependencies\n",
    "\n",
    "This cell installs all required packages (~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check GPU availability\n",
    "echo \"=== GPU Check ===\"\n",
    "nvidia-smi || echo \"‚ö†Ô∏è No GPU detected. Pipeline will use CPU (slower).\"\n",
    "\n",
    "# Install system dependencies\n",
    "echo -e \"\\n=== Installing System Dependencies ===\"\n",
    "apt-get update -qq\n",
    "apt-get install -y -qq dcm2niix pigz > /dev/null\n",
    "\n",
    "# Install Python packages\n",
    "echo -e \"\\n=== Installing Python Packages ===\"\n",
    "pip install -q --upgrade pip\n",
    "pip install -q pydicom dicompyler-core numpy pandas scipy SimpleITK \\\n",
    "    dicom2nifti rt-utils plotly matplotlib openpyxl xlsxwriter \\\n",
    "    TotalSegmentator>=2.4.0 snakemake\n",
    "\n",
    "# Install PyRadiomics (separate due to numpy compatibility)\n",
    "pip install -q \"numpy<2.0\" pyradiomics\n",
    "\n",
    "echo -e \"\\n‚úÖ Setup complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## 2Ô∏è‚É£ Clone RTpipeline Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repository\n",
    "if [ ! -d \"/content/rtpipeline\" ]; then\n",
    "    echo \"Cloning rtpipeline repository...\"\n",
    "    git clone https://github.com/kstawiski/rtpipeline.git /content/rtpipeline\n",
    "    echo \"‚úÖ Repository cloned\"\n",
    "else\n",
    "    echo \"‚úÖ Repository already exists\"\n",
    "fi\n",
    "\n",
    "cd /content/rtpipeline\n",
    "git pull origin main\n",
    "echo \"Repository updated to latest version\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## 3Ô∏è‚É£ Upload Your DICOM Files\n",
    "\n",
    "You have two options:\n",
    "\n",
    "### Option A: Upload from Google Drive\n",
    "\n",
    "Run this cell to mount your Google Drive, then access files from `/content/drive/MyDrive/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted at /content/drive/MyDrive/\")\n",
    "print(\"\\nYour DICOM files should be in: /content/drive/MyDrive/your_dicom_folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_option_b"
   },
   "source": [
    "### Option B: Upload Files Directly\n",
    "\n",
    "Use the file browser on the left (üìÅ icon) to upload DICOM files to `/content/dicom_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dirs"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create directories\n",
    "mkdir -p /content/dicom_data\n",
    "mkdir -p /content/output\n",
    "mkdir -p /content/logs\n",
    "\n",
    "echo \"‚úÖ Directories created:\"\n",
    "echo \"  - /content/dicom_data (upload your DICOM files here)\"\n",
    "echo \"  - /content/output (results will be saved here)\"\n",
    "echo \"  - /content/logs (processing logs)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 4Ô∏è‚É£ Configure Pipeline\n",
    "\n",
    "Modify the settings below according to your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_config"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============ CONFIGURATION ============\n",
    "\n",
    "# DICOM directory (change if using Google Drive)\n",
    "DICOM_ROOT = \"/content/dicom_data\"\n",
    "# Example: DICOM_ROOT = \"/content/drive/MyDrive/my_dicom_folder\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "LOGS_DIR = \"/content/logs\"\n",
    "\n",
    "# Processing options\n",
    "USE_GPU = True  # Set to False if no GPU available\n",
    "ENABLE_RADIOMICS = True  # Extract radiomic features\n",
    "ENABLE_CT_CROPPING = False  # Crop CT to anatomical region\n",
    "CROP_REGION = \"pelvis\"  # Options: pelvis, thorax, abdomen, head_neck, brain\n",
    "\n",
    "# Advanced settings\n",
    "WORKERS = 4  # Parallel workers (adjust based on available memory)\n",
    "SEG_WORKERS = 2  # Segmentation workers (GPU: 1-4, CPU: 1)\n",
    "FAST_MODE = False  # CPU-friendly mode (lower quality)\n",
    "\n",
    "# =======================================\n",
    "\n",
    "# Detect GPU\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.run(['nvidia-smi'], check=True, capture_output=True)\n",
    "    gpu_available = True\n",
    "    print(\"‚úÖ GPU detected\")\n",
    "except:\n",
    "    gpu_available = False\n",
    "    USE_GPU = False\n",
    "    SEG_WORKERS = 1\n",
    "    print(\"‚ö†Ô∏è No GPU detected - using CPU mode\")\n",
    "\n",
    "# Check DICOM directory\n",
    "if not os.path.exists(DICOM_ROOT):\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: DICOM directory not found: {DICOM_ROOT}\")\n",
    "    print(\"Please upload your DICOM files or update DICOM_ROOT variable above.\")\n",
    "else:\n",
    "    dicom_count = sum(1 for root, dirs, files in os.walk(DICOM_ROOT) for f in files if f.endswith('.dcm'))\n",
    "    print(f\"\\n‚úÖ DICOM directory found: {DICOM_ROOT}\")\n",
    "    print(f\"   Found {dicom_count} DICOM files\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  GPU: {USE_GPU}\")\n",
    "print(f\"  Radiomics: {ENABLE_RADIOMICS}\")\n",
    "print(f\"  CT Cropping: {ENABLE_CT_CROPPING}\")\n",
    "print(f\"  Workers: {WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate_config"
   },
   "source": [
    "## 5Ô∏è‚É£ Generate Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_config"
   },
   "outputs": [],
   "source": [
    "config_yaml = f\"\"\"# RTpipeline Configuration for Google Colab\n",
    "# Generated automatically\n",
    "\n",
    "# Input/Output directories\n",
    "dicom_root: \"{DICOM_ROOT}\"\n",
    "output_dir: \"{OUTPUT_DIR}\"\n",
    "logs_dir: \"{LOGS_DIR}\"\n",
    "\n",
    "# Processing parameters\n",
    "workers: {WORKERS}\n",
    "\n",
    "segmentation:\n",
    "  workers: {SEG_WORKERS}\n",
    "  threads_per_worker: null\n",
    "  force: false\n",
    "  fast: {str(FAST_MODE).lower()}\n",
    "  roi_subset: null\n",
    "  extra_models: []\n",
    "  device: \"{'gpu' if USE_GPU else 'cpu'}\"\n",
    "  force_split: true\n",
    "  nr_threads_resample: 1\n",
    "  nr_threads_save: 1\n",
    "  num_proc_preprocessing: 1\n",
    "  num_proc_export: 1\n",
    "\n",
    "custom_models:\n",
    "  enabled: false\n",
    "  root: \"custom_models\"\n",
    "  models: []\n",
    "  workers: 1\n",
    "  force: false\n",
    "  nnunet_predict: \"nnUNet_predict\"\n",
    "  retain_weights: true\n",
    "  conda_activate: null\n",
    "\n",
    "radiomics:\n",
    "  sequential: false\n",
    "  params_file: \"/content/rtpipeline/rtpipeline/radiomics_params.yaml\"\n",
    "  mr_params_file: \"/content/rtpipeline/rtpipeline/radiomics_params_mr.yaml\"\n",
    "  thread_limit: 4\n",
    "  skip_rois:\n",
    "    - body\n",
    "    - couchsurface\n",
    "    - bones\n",
    "  max_voxels: 1500000000\n",
    "  min_voxels: 10\n",
    "\n",
    "aggregation:\n",
    "  threads: auto\n",
    "\n",
    "environments:\n",
    "  main: \"base\"\n",
    "  radiomics: \"base\"\n",
    "\n",
    "custom_structures: \"custom_structures_pelvic.yaml\"\n",
    "\n",
    "ct_cropping:\n",
    "  enabled: {str(ENABLE_CT_CROPPING).lower()}\n",
    "  region: \"{CROP_REGION}\"\n",
    "  superior_margin_cm: 2.0\n",
    "  inferior_margin_cm: 10.0\n",
    "  use_cropped_for_dvh: true\n",
    "  use_cropped_for_radiomics: true\n",
    "  keep_original: true\n",
    "\"\"\"\n",
    "\n",
    "# Write config file\n",
    "config_path = \"/content/config_colab.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "print(f\"‚úÖ Configuration written to: {config_path}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(config_yaml[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run"
   },
   "source": [
    "## 6Ô∏è‚É£ Run Pipeline\n",
    "\n",
    "‚è±Ô∏è **Estimated time:**\n",
    "- With GPU: 10-30 minutes per patient\n",
    "- Without GPU: 1-3 hours per patient\n",
    "\n",
    "**Note:** Colab may timeout after 12 hours. For large datasets, process in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /content/rtpipeline\n",
    "\n",
    "echo \"=== Starting RTpipeline ===\"\n",
    "echo \"Configuration: /content/config_colab.yaml\"\n",
    "echo \"\"\n",
    "\n",
    "# Run pipeline using Python CLI (simpler than Snakemake for Colab)\n",
    "python3 -m rtpipeline.cli \\\n",
    "    --dicom-root \"${DICOM_ROOT}\" \\\n",
    "    --outdir \"${OUTPUT_DIR}\" \\\n",
    "    --logs \"${LOGS_DIR}\" \\\n",
    "    --workers ${WORKERS} \\\n",
    "    --seg-workers ${SEG_WORKERS}\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Pipeline Complete ===\"\n",
    "echo \"Results saved to: ${OUTPUT_DIR}\"\n",
    "echo \"Check aggregated results: ${OUTPUT_DIR}/_RESULTS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 7Ô∏è‚É£ View Results\n",
    "\n",
    "Load and preview the aggregated results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_results"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "results_dir = f\"{OUTPUT_DIR}/_RESULTS\"\n",
    "\n",
    "# Check if results exist\n",
    "if not os.path.exists(results_dir):\n",
    "    print(\"‚ö†Ô∏è Results directory not found. Pipeline may still be running or failed.\")\n",
    "    print(f\"Expected location: {results_dir}\")\n",
    "else:\n",
    "    print(\"‚úÖ Results found!\\n\")\n",
    "\n",
    "    # List result files\n",
    "    result_files = os.listdir(results_dir)\n",
    "    print(\"Available files:\")\n",
    "    for f in result_files:\n",
    "        if f.endswith('.xlsx'):\n",
    "            filepath = os.path.join(results_dir, f)\n",
    "            size_mb = os.path.getsize(filepath) / 1024 / 1024\n",
    "            print(f\"  - {f} ({size_mb:.2f} MB)\")\n",
    "\n",
    "    # Load DVH metrics\n",
    "    try:\n",
    "        dvh_path = os.path.join(results_dir, \"dvh_metrics.xlsx\")\n",
    "        dvh = pd.read_excel(dvh_path)\n",
    "        print(f\"\\n‚úÖ Loaded DVH metrics: {len(dvh)} rows\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        display(dvh.head())\n",
    "\n",
    "        print(\"\\nStructures found:\")\n",
    "        print(dvh['Structure'].value_counts().head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not load DVH metrics: {e}\")\n",
    "\n",
    "    # Load radiomics\n",
    "    if ENABLE_RADIOMICS:\n",
    "        try:\n",
    "            rad_path = os.path.join(results_dir, \"radiomics_ct.xlsx\")\n",
    "            radiomics = pd.read_excel(rad_path)\n",
    "            print(f\"\\n‚úÖ Loaded radiomics: {len(radiomics)} rows, {len(radiomics.columns)} features\")\n",
    "            print(\"\\nFirst few rows:\")\n",
    "            display(radiomics.head())\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Could not load radiomics: {e}\")\n",
    "\n",
    "    # Load metadata\n",
    "    try:\n",
    "        meta_path = os.path.join(results_dir, \"case_metadata.xlsx\")\n",
    "        metadata = pd.read_excel(meta_path)\n",
    "        print(f\"\\n‚úÖ Loaded metadata: {len(metadata)} courses\")\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"  Patients: {metadata['PatientID'].nunique()}\")\n",
    "        print(f\"  Courses: {len(metadata)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Could not load metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "## 8Ô∏è‚É£ Quick Visualization\n",
    "\n",
    "Create some basic plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# DVH metrics visualization\n",
    "try:\n",
    "    # Plot mean dose by structure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Mean dose\n",
    "    top_structures = dvh.groupby('Structure')['Dmean_Gy'].mean().sort_values(ascending=False).head(10)\n",
    "    top_structures.plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_xlabel('Mean Dose (Gy)')\n",
    "    axes[0].set_title('Top 10 Structures by Mean Dose')\n",
    "\n",
    "    # Volume distribution\n",
    "    dvh['ROI_Volume_cc'].hist(bins=50, ax=axes[1], color='coral', edgecolor='black')\n",
    "    axes[1].set_xlabel('ROI Volume (cc)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('ROI Volume Distribution')\n",
    "    axes[1].set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualizations created\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not create visualizations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 9Ô∏è‚É£ Download Results\n",
    "\n",
    "Download results to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_results"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create ZIP archive of results\n",
    "echo \"Creating results archive...\"\n",
    "cd /content\n",
    "zip -r -q results.zip output/_RESULTS/\n",
    "\n",
    "echo \"‚úÖ Results archived: /content/results.zip\"\n",
    "ls -lh /content/results.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_zip"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading results.zip...\")\n",
    "files.download('/content/results.zip')\n",
    "print(\"\\n‚úÖ Download started. Check your browser's download folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_drive"
   },
   "source": [
    "### Alternative: Save to Google Drive\n",
    "\n",
    "If you mounted Google Drive earlier, copy results there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy_to_drive"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check if Drive is mounted\n",
    "if [ -d \"/content/drive/MyDrive\" ]; then\n",
    "    echo \"Copying results to Google Drive...\"\n",
    "    cp -r /content/output/_RESULTS /content/drive/MyDrive/rtpipeline_results_$(date +%Y%m%d_%H%M%S)\n",
    "    echo \"‚úÖ Results copied to: /content/drive/MyDrive/rtpipeline_results_*\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è Google Drive not mounted. Run the 'Mount Google Drive' cell first.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## üßπ Cleanup (Optional)\n",
    "\n",
    "Free up space by removing large intermediate files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup_files"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Disk usage before cleanup:\"\n",
    "du -sh /content/output\n",
    "\n",
    "# Remove intermediate segmentation files (keep only _RESULTS)\n",
    "# Uncomment to clean:\n",
    "# find /content/output -type d -name \"Segmentation_*\" -exec rm -rf {} + 2>/dev/null\n",
    "# find /content/output -type f -name \"*.nii.gz\" -delete 2>/dev/null\n",
    "\n",
    "echo \"\\nTo cleanup, uncomment the find commands in this cell and re-run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "help"
   },
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **Output Format Guide:** [output_format.md](https://github.com/kstawiski/rtpipeline/blob/main/output_format.md)\n",
    "- **Quick Reference:** [output_format_quick_ref.md](https://github.com/kstawiski/rtpipeline/blob/main/output_format_quick_ref.md)\n",
    "- **GitHub Repository:** https://github.com/kstawiski/rtpipeline\n",
    "- **Issues/Questions:** https://github.com/kstawiski/rtpipeline/issues\n",
    "\n",
    "## ‚ö†Ô∏è Troubleshooting\n",
    "\n",
    "**Pipeline fails with GPU errors:**\n",
    "- Set `USE_GPU = False` in configuration cell\n",
    "- Reduce `SEG_WORKERS` to 1\n",
    "\n",
    "**Out of memory errors:**\n",
    "- Reduce `WORKERS` and `SEG_WORKERS`\n",
    "- Enable `FAST_MODE = True`\n",
    "- Process patients in smaller batches\n",
    "\n",
    "**Colab timeout:**\n",
    "- Upgrade to Colab Pro for longer runtime\n",
    "- Process in batches\n",
    "- Save intermediate results to Google Drive\n",
    "\n",
    "**Missing DICOM files:**\n",
    "- Ensure DICOM directory is correct\n",
    "- Check file permissions\n",
    "- Verify .dcm file extensions\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Version:** 1.0\n",
    "**Compatible with:** rtpipeline v2.0+\n",
    "**Last Updated:** 2025-11-13\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

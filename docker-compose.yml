version: '3.8'

services:
  # Main rtpipeline service with GPU support (DEFAULT)
  rtpipeline:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
    image: kstawiski/rtpipeline:latest
    container_name: rtpipeline
    hostname: rtpipeline
    volumes:
      # Mount your DICOM input data
      - ./Input:/data/input:rw
      # Mount output directory for pipeline results
      - ./Output:/data/output:rw
      # Mount logs directory
      - ./Logs:/data/logs:rw
      # Mount custom models directory (for nnUNet weights)
      - ./custom_models:/data/models:rw
      # Mount uploads directory for web UI
      - ./Uploads:/data/uploads:rw
      # Optional: mount notebooks for development
      - ./Code:/app/Code:rw
    ports:
      # Web UI port
      - "8080:8080"
    environment:
      # Pipeline configuration
      - PYTHONPATH=/app
      - NUMBA_CACHE_DIR=/tmp/cache
      - MPLCONFIGDIR=/tmp/cache
      # Conda environment
      - CONDA_DIR=/opt/conda
      - PATH=/opt/conda/bin:$PATH
      # GPU support (DEFAULT)
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
      # Web UI configuration
      - PORT=8080
      - DEBUG=false
    working_dir: /app
    # Start web UI by default
    command: >
      bash -c "
        cd /app/webui &&
        python app.py
      "
    # GPU support - DEFAULT configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          cpus: '4.0'
          memory: 8G
        limits:
          cpus: '24.0'
          memory: 64G
    shm_size: '4gb'  # Increased shared memory for PyTorch
    restart: unless-stopped

  # CPU-only service (use --profile cpu-only to activate)
  rtpipeline-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
    image: kstawiski/rtpipeline:latest
    container_name: rtpipeline-cpu
    hostname: rtpipeline-cpu
    volumes:
      - ./Input:/data/input:rw
      - ./Output:/data/output:rw
      - ./Logs:/data/logs:rw
      - ./custom_models:/data/models:rw
      - ./Uploads:/data/uploads:rw
      - ./Code:/app/Code:rw
    ports:
      - "8080:8080"
    environment:
      - PYTHONPATH=/app
      - NUMBA_CACHE_DIR=/tmp/cache
      - MPLCONFIGDIR=/tmp/cache
      - CONDA_DIR=/opt/conda
      - PATH=/opt/conda/bin:$PATH
      - PORT=8080
      - DEBUG=false
    working_dir: /app
    command: >
      bash -c "
        cd /app/webui &&
        python app.py
      "
    # CPU-only: no GPU resources requested
    deploy:
      resources:
        limits:
          cpus: '16.0'
          memory: 32G
        reservations:
          cpus: '4.0'
          memory: 8G
    shm_size: '4gb'
    restart: unless-stopped
    profiles:
      - cpu-only

  # Jupyter notebook service for interactive analysis
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    image: kstawiski/rtpipeline:latest
    container_name: rtpipeline-jupyter
    volumes:
      - ./Code:/app/Code:rw
      - ./Output:/data/output:ro
      - ./Input:/data/input:ro
      - ./custom_models:/data/models:ro
    environment:
      - PYTHONPATH=/app
      - CONDA_DIR=/opt/conda
      - PATH=/opt/conda/bin:$PATH
      # GPU support for Jupyter
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    working_dir: /app/Code
    ports:
      - "8888:8888"
    command: >
      bash -c "
        conda run -n rtpipeline jupyter lab \
          --ip=0.0.0.0 \
          --port=8888 \
          --no-browser \
          --allow-root
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          cpus: '8.0'
          memory: 16G
    profiles:
      - jupyter

networks:
  default:
    name: rtpipeline-network

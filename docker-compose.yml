version: '3.8'

services:
  # Main rtpipeline service with GPU support
  rtpipeline:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
    image: rtpipeline:latest
    container_name: rtpipeline
    hostname: rtpipeline
    volumes:
      # Mount DICOM input data (read-only)
      - ./Example_data:/app/Example_data:ro
      # Mount output directory for pipeline results
      - ./Data_Snakemake:/app/Data_Snakemake:rw
      # Mount logs directory
      - ./Logs_Snakemake:/app/Logs_Snakemake:rw
      # Mount custom models directory
      - ./custom_models:/app/custom_models:rw
      # Mount configuration
      - ./config.yaml:/app/config.yaml:ro
      # Optional: mount custom structures
      - ./custom_structures_pelvic.yaml:/app/custom_structures_pelvic.yaml:ro
      # Optional: mount notebooks for development
      - ./Code:/app/Code:rw
    environment:
      # Pipeline configuration
      - PYTHONPATH=/app
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - MPLCONFIGDIR=/tmp/matplotlib
      # Conda environment
      - CONDA_DIR=/opt/conda
      - PATH=/opt/conda/bin:$PATH
      # GPU support (will be ignored if no GPU available)
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    working_dir: /app
    # Keep container running for interactive use
    stdin_open: true
    tty: true
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '16.0'
          memory: 32G
        reservations:
          cpus: '4.0'
          memory: 8G
    # Uncomment for GPU support (requires nvidia-docker or Docker with GPU support)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    shm_size: '4gb'  # Increased shared memory for PyTorch
    restart: unless-stopped

  # GPU-enabled service (requires nvidia-docker)
  rtpipeline-gpu:
    extends:
      service: rtpipeline
    image: rtpipeline:latest
    container_name: rtpipeline-gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          cpus: '24.0'
          memory: 64G
    environment:
      - PYTHONPATH=/app
      - NUMBA_CACHE_DIR=/tmp/numba_cache
      - MPLCONFIGDIR=/tmp/matplotlib
      - CONDA_DIR=/opt/conda
      - PATH=/opt/conda/bin:$PATH
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
    profiles:
      - gpu

  # Jupyter notebook service for interactive analysis
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    image: rtpipeline:latest
    container_name: rtpipeline-jupyter
    volumes:
      - ./Code:/app/Code:rw
      - ./Data_Snakemake:/app/Data_Snakemake:ro
      - ./Example_data:/app/Example_data:ro
      - ./custom_models:/app/custom_models:ro
      - ./config.yaml:/app/config.yaml:ro
    environment:
      - PYTHONPATH=/app
      - CONDA_DIR=/opt/conda
      - PATH=/opt/conda/bin:$PATH
    working_dir: /app/Code
    ports:
      - "8888:8888"
    command: >
      bash -c "
        conda run -n rtpipeline jupyter lab \
          --ip=0.0.0.0 \
          --port=8888 \
          --no-browser \
          --allow-root \
          --NotebookApp.token='' \
          --NotebookApp.password=''
      "
    deploy:
      resources:
        limits:
          cpus: '8.0'
          memory: 16G
    profiles:
      - jupyter

networks:
  default:
    name: rtpipeline-network

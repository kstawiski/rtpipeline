{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b558e50d-bf1a-4729-992d-12608befb304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dicompyler-core\n",
      "  Downloading dicompyler_core-0.5.6-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting nested-lookup\n",
      "  Downloading nested-lookup-0.2.25.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pydicom==1.4.2\n",
      "  Downloading pydicom-1.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting numpy>=1.2 (from dicompyler-core)\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/jupyter_env/lib/python3.10/site-packages (from dicompyler-core) (1.17.0)\n",
      "Collecting matplotlib>=1.3.0 (from dicompyler-core)\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=1.3.0->dicompyler-core)\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=1.3.0->dicompyler-core)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=1.3.0->dicompyler-core)\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=1.3.0->dicompyler-core)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib>=1.3.0->dicompyler-core) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib>=1.3.0->dicompyler-core)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=1.3.0->dicompyler-core)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/jupyter_env/lib/python3.10/site-packages (from matplotlib>=1.3.0->dicompyler-core) (2.9.0.post0)\n",
      "Downloading pydicom-1.4.2-py2.py3-none-any.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "Downloading dicompyler_core-0.5.6-py2.py3-none-any.whl (34 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Building wheels for collected packages: nested-lookup\n",
      "  Building wheel for nested-lookup (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nested-lookup: filename=nested_lookup-0.2.25-py3-none-any.whl size=13215 sha256=a6b696d0ac83cbb2d0ad98ae5499bad4e270cd9ee0201e0c7e64e7d21bf15629\n",
      "  Stored in directory: /root/.cache/pip/wheels/f9/d6/89/e04a9116dbc5fe6fd7112b9d60051224595d8370433fb43759\n",
      "Successfully built nested-lookup\n",
      "Installing collected packages: pydicom, pyparsing, pillow, numpy, nested-lookup, kiwisolver, fonttools, et-xmlfile, cycler, openpyxl, contourpy, matplotlib, dicompyler-core\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 dicompyler-core-0.5.6 et-xmlfile-2.0.0 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 nested-lookup-0.2.25 numpy-2.2.3 openpyxl-3.1.5 pillow-11.1.0 pydicom-1.4.2 pyparsing-3.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install dicompyler-core nested-lookup openpyxl pydicom==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d952b-6bb6-42fe-9668-52adf6ae9999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CT files:   2%|▎              | 1550/80569 [01:27<1:16:07, 17.30it/s]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "import fnmatch\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dicompylercore import dicomparser\n",
    "from nested_lookup import nested_lookup\n",
    "from typing import List, Dict, Any, Generator, Set\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import hashlib\n",
    "import json  # new import for JSON handling\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DicomSeriesOrganizer:\n",
    "    \"\"\"Class to handle DICOM file processing and organization operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_directory: str, output_base: str):\n",
    "        \"\"\"\n",
    "        Initialize the DicomSeriesOrganizer.\n",
    "        \n",
    "        Args:\n",
    "            base_directory (str): Root directory containing DICOM files\n",
    "            output_base (str): Base directory for organized output\n",
    "        \"\"\"\n",
    "        self.base_directory = Path(base_directory)\n",
    "        self.output_base = Path(output_base)\n",
    "        self.required_packages = [\n",
    "            'dicompyler-core',\n",
    "            'nested-lookup',\n",
    "            'openpyxl',\n",
    "            'pydicom'  # removed version constraint to use the latest version\n",
    "        ]\n",
    "        \n",
    "    def get_file_hash(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Calculate MD5 hash of file to identify duplicates.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to file\n",
    "            \n",
    "        Returns:\n",
    "            str: MD5 hash of file\n",
    "        \"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_lookup(tag: str, dicom_plan: Any, index: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        Safely extract DICOM tag value.\n",
    "        \n",
    "        Args:\n",
    "            tag (str): DICOM tag to look up\n",
    "            dicom_plan: DicomParser object\n",
    "            index (int): Index for nested lookup\n",
    "            \n",
    "        Returns:\n",
    "            str: Tag value or \"NA\" if not found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert the Dataset to JSON string and then parse into a dict.\n",
    "            ds_json = json.loads(dicom_plan.ds.to_json())\n",
    "            return nested_lookup(tag, ds_json)[index]['Value'][0]\n",
    "        except (IndexError, KeyError, AttributeError):\n",
    "            return \"NA\"\n",
    "\n",
    "    def find_ct_files(self, pattern: str = 'CT*.dcm') -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Find CT DICOM files in directory and subdirectories.\n",
    "        \n",
    "        Args:\n",
    "            pattern (str): File pattern to match\n",
    "            \n",
    "        Yields:\n",
    "            str: Path to matching file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for root, _, files in os.walk(self.base_directory):\n",
    "                for filename in fnmatch.filter(files, pattern):\n",
    "                    yield os.path.join(root, filename)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching for CT files: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_destination_path(self, patient_id: str, series_number: str, instance_number: str) -> Path:\n",
    "        \"\"\"\n",
    "        Generate destination path for organized file structure.\n",
    "        \n",
    "        Args:\n",
    "            patient_id (str): Patient identifier\n",
    "            series_number (str): Series number\n",
    "            instance_number (str): Instance number\n",
    "            \n",
    "        Returns:\n",
    "            Path: Destination path for file\n",
    "        \"\"\"\n",
    "        return self.output_base / str(patient_id) / f\"series_{series_number}\" / f\"CT_{instance_number}.dcm\"\n",
    "\n",
    "    def process_ct_files(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process CT files and remove duplicates based on file content.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing unique CT file information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            paths = list(self.find_ct_files())\n",
    "            if not paths:\n",
    "                logger.warning(\"No CT files found\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            data = []\n",
    "            seen_hashes: Set[str] = set()\n",
    "            \n",
    "            for path in tqdm(paths, desc=\"Processing CT files\"):\n",
    "                try:\n",
    "                    # Calculate file hash\n",
    "                    file_hash = self.get_file_hash(path)\n",
    "                    \n",
    "                    # Skip if we've seen this file before\n",
    "                    if file_hash in seen_hashes:\n",
    "                        logger.info(f\"Skipping duplicate file: {path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    seen_hashes.add(file_hash)\n",
    "                    \n",
    "                    # Process DICOM file\n",
    "                    dicomrt_file = dicomparser.DicomParser(path)\n",
    "                    \n",
    "                    # Get DICOM tags\n",
    "                    patient_id = self.safe_lookup(\"00100020\", dicomrt_file)\n",
    "                    series_number = self.safe_lookup(\"00200011\", dicomrt_file)\n",
    "                    instance_number = self.safe_lookup(\"00200013\", dicomrt_file)\n",
    "                    \n",
    "                    # Generate destination path\n",
    "                    dest_path = self.get_destination_path(patient_id, series_number, instance_number)\n",
    "                    \n",
    "                    row_info = {\n",
    "                        \"original_path\": path,\n",
    "                        \"organized_path\": str(dest_path),\n",
    "                        \"CT_series\": self.safe_lookup(\"0020000E\", dicomrt_file),\n",
    "                        \"CT_study\": self.safe_lookup(\"0020000D\", dicomrt_file),\n",
    "                        \"PatientID\": patient_id,\n",
    "                        \"SeriesNumber\": series_number,\n",
    "                        \"InstanceNumber\": instance_number,\n",
    "                        \"file_hash\": file_hash\n",
    "                    }\n",
    "                    data.append(row_info)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing file {path}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_ct_files: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def organize_files(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Organize DICOM files into patient/series structure.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing CT file information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for _, row in tqdm(df.iterrows(), desc=\"Organizing files\", total=len(df)):\n",
    "                try:\n",
    "                    # Get destination path\n",
    "                    destination = Path(row['organized_path'])\n",
    "                    \n",
    "                    # Create directory structure\n",
    "                    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    # Copy file to new location\n",
    "                    shutil.copy2(row['original_path'], destination)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error organizing file {row['original_path']}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            logger.info(\"File organization complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in organize_files: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        # Initialize paths\n",
    "        input_dir = \"../DICOM\"\n",
    "        output_dir = \"../DICOM_OrganizedCT\"\n",
    "        excel_path = \"../Data/CT_images.xlsx\"\n",
    "        csv_path = \"../Data/CT_images.csv\"\n",
    "        \n",
    "        # Initialize organizer\n",
    "        organizer = DicomSeriesOrganizer(input_dir, output_dir)\n",
    "        \n",
    "        # Process files and remove duplicates\n",
    "        df = organizer.process_ct_files()\n",
    "        \n",
    "        # Save to Excel and CSV\n",
    "        df.to_excel(excel_path, index=False)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"Saved {len(df)} unique files information to {excel_path}\")\n",
    "        \n",
    "        # Organize files into patient/series structure\n",
    "        organizer.organize_files(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main execution error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74aeba-185b-4d3d-aee4-91d461b9a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 02:41:54,134 - INFO - Processing plans\n",
      "2025-02-28 02:42:28,714 - INFO - Processing 252 files matching pattern RP*.dcm\n",
      "2025-02-28 02:43:44,801 - INFO - Skipping duplicate file: ../DICOM/438308/RP.438308.odbytnica.dcm\n",
      "Processing RP*.dcm:  56%|███████████▊         | 141/252 [01:22<01:19,  1.40it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import fnmatch\n",
    "from typing import List, Dict, Any, Generator, Set, Tuple\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nested_lookup import nested_lookup\n",
    "from dicompylercore import dicomparser\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('dicom_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DicomProcessor:\n",
    "    \"\"\"Class to handle DICOM file processing for radiotherapy data.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir: str, output_dir: str):\n",
    "        \"\"\"Initialize the processor with input and output directories.\"\"\"\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.processed_files: Dict[str, Set[str]] = {\n",
    "            'plans': set(),\n",
    "            'fractions': set(),\n",
    "            'structure_sets': set(),\n",
    "            'dosimetrics': set()\n",
    "        }\n",
    "        self.validate_directories()\n",
    "        \n",
    "    def validate_directories(self) -> None:\n",
    "        \"\"\"Validate and create directories if needed.\"\"\"\n",
    "        if not self.input_dir.exists():\n",
    "            raise ValueError(f\"Input directory does not exist: {self.input_dir}\")\n",
    "        \n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    @staticmethod\n",
    "    def find_files(directory: str, pattern: str) -> Generator[str, None, None]:\n",
    "        \"\"\"Find files matching pattern in directory.\"\"\"\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for filename in fnmatch.filter(files, pattern):\n",
    "                yield os.path.join(root, filename)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_lookup(tag: str, dicom_file: dicomparser.DicomParser, index: int = 0) -> str:\n",
    "        \"\"\"Safely extract DICOM tag value with enhanced error handling.\"\"\"\n",
    "        try:\n",
    "            return str(nested_lookup(tag, dicom_file.ds.to_json_dict())[index]['Value'][0])\n",
    "        except (IndexError, KeyError) as e:\n",
    "            logger.debug(f\"Tag {tag} not found: {str(e)}\")\n",
    "            return \"NA\"\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Unexpected error accessing tag {tag}: {str(e)}\")\n",
    "            return \"NA\"\n",
    "\n",
    "    def generate_file_hash(self, path: str, key_tags: List[str]) -> str:\n",
    "        \"\"\"Generate a unique hash for a DICOM file based on key identifying tags.\"\"\"\n",
    "        try:\n",
    "            dicom_file = dicomparser.DicomParser(path)\n",
    "            hash_input = []\n",
    "            \n",
    "            for tag in key_tags:\n",
    "                value = self.safe_lookup(tag, dicom_file)\n",
    "                hash_input.append(str(value))\n",
    "            \n",
    "            return hashlib.md5(''.join(hash_input).encode()).hexdigest()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating hash for {path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def is_duplicate(self, file_type: str, file_hash: str) -> bool:\n",
    "        \"\"\"Check if a file has already been processed based on its hash.\"\"\"\n",
    "        return file_hash in self.processed_files[file_type]\n",
    "\n",
    "    def process_file(self, path: str, tags_dict: Dict[str, str], \n",
    "                    file_type: str, key_tags: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Process a single DICOM file and extract specified tags.\"\"\"\n",
    "        try:\n",
    "            # Generate hash based on key identifying tags\n",
    "            file_hash = self.generate_file_hash(path, key_tags)\n",
    "            if not file_hash:\n",
    "                return None\n",
    "                \n",
    "            # Check for duplicates\n",
    "            if self.is_duplicate(file_type, file_hash):\n",
    "                logger.info(f\"Skipping duplicate file: {path}\")\n",
    "                return None\n",
    "                \n",
    "            # Process the file\n",
    "            dicom_file = dicomparser.DicomParser(path)\n",
    "            result = {\"file_path\": path}\n",
    "            \n",
    "            for key, tag in tags_dict.items():\n",
    "                result[key] = self.safe_lookup(tag, dicom_file)\n",
    "            \n",
    "            # Add the hash to processed files\n",
    "            self.processed_files[file_type].add(file_hash)\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_files(self, file_pattern: str, tags_dict: Dict[str, str], \n",
    "                     output_name: str, key_tags: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Process all files matching pattern and save results.\"\"\"\n",
    "        paths = list(self.find_files(self.input_dir, file_pattern))\n",
    "        data = []\n",
    "        \n",
    "        logger.info(f\"Processing {len(paths)} files matching pattern {file_pattern}\")\n",
    "        \n",
    "        for path in tqdm(paths, desc=f\"Processing {file_pattern}\"):\n",
    "            result = self.process_file(path, tags_dict, output_name, key_tags)\n",
    "            if result:\n",
    "                data.append(result)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Add processing metadata\n",
    "            df['processing_timestamp'] = datetime.now().isoformat()\n",
    "            df['file_pattern'] = file_pattern\n",
    "            \n",
    "            # Save to both Excel and CSV\n",
    "            output_base = self.output_dir / output_name\n",
    "            df.to_excel(f\"{output_base}.xlsx\", index=False)\n",
    "            df.to_csv(f\"{output_base}.csv\", index=False)\n",
    "            \n",
    "            # Log duplicate statistics\n",
    "            total_files = len(paths)\n",
    "            unique_files = len(data)\n",
    "            duplicates = total_files - unique_files\n",
    "            logger.info(f\"Found {duplicates} duplicate files out of {total_files} total files for {output_name}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_all(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Process all DICOM file types and return results.\"\"\"\n",
    "        # Define tags for each file type\n",
    "        tags = {\n",
    "            \"plans\": {\n",
    "                \"plan_name\": \"300A0002\",\n",
    "                \"plan_date\": \"300A0006\",\n",
    "                \"reference_dose_name\": \"300A0016\",\n",
    "                \"reference_dose\": \"300A0026\",\n",
    "                \"approval\": \"300E0002\",\n",
    "                \"CT_series\": \"0020000E\",\n",
    "                \"CT_study\": \"0020000D\",\n",
    "                \"patient_id\": \"00100020\",\n",
    "                \"patient_dob\": \"00100030\",\n",
    "                \"patient_gender\": \"00100040\",\n",
    "                \"patient_pesel\": \"00101000\"\n",
    "            },\n",
    "            \"fractions\": {\n",
    "                \"fraction_id\": \"00080018\",\n",
    "                \"date\": \"30080024\",\n",
    "                \"time\": \"30080025\",\n",
    "                \"fraction_number\": \"30080022\",\n",
    "                \"verification_status\": \"3008002C\",\n",
    "                \"termination_status\": \"3008002A\",\n",
    "                \"delivery_time\": \"3008003B\",\n",
    "                \"fluence_mode\": \"30020052\",\n",
    "                \"plan_id\": \"00081155\",\n",
    "                \"machine\": \"300A00B2\",\n",
    "                \"patient_id\": \"00100020\"\n",
    "            },\n",
    "            \"structure_sets\": {\n",
    "                \"CT_series\": \"0020000E\",\n",
    "                \"CT_study\": \"0020000D\",\n",
    "                \"approval\": \"300E0002\",\n",
    "                \"patient_id\": \"00100020\"\n",
    "            },\n",
    "            \"dosimetrics\": {\n",
    "                \"CT_series\": \"0020000E\",\n",
    "                \"CT_study\": \"0020000D\",\n",
    "                \"plan_id\": \"00081155\",\n",
    "                \"patient_id\": \"00100020\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Define key identifying tags for each file type to detect duplicates\n",
    "        key_tags = {\n",
    "            \"plans\": [\"00100020\", \"300A0002\", \"300A0006\"],  # patient_id, plan_name, plan_date\n",
    "            \"fractions\": [\"00100020\", \"00080018\", \"30080022\"],  # patient_id, fraction_id, fraction_number\n",
    "            \"structure_sets\": [\"00100020\", \"0020000E\", \"0020000D\"],  # patient_id, CT_series, CT_study\n",
    "            \"dosimetrics\": [\"00100020\", \"00081155\", \"0020000E\"]  # patient_id, plan_id, CT_series\n",
    "        }\n",
    "        \n",
    "        # Process each file type\n",
    "        results = {}\n",
    "        file_patterns = {\n",
    "            \"plans\": \"RP*.dcm\",\n",
    "            \"fractions\": \"RT*.dcm\",\n",
    "            \"structure_sets\": \"RS*.dcm\",\n",
    "            \"dosimetrics\": \"RD*.dcm\"\n",
    "        }\n",
    "        \n",
    "        for name, pattern in file_patterns.items():\n",
    "            logger.info(f\"Processing {name}\")\n",
    "            results[name] = self.process_files(pattern, tags[name], name, key_tags[name])\n",
    "            \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        processor = DicomProcessor(\n",
    "            input_dir = \"../DICOM/\",\n",
    "            output_dir = \"../Data/\"\n",
    "        )\n",
    "        \n",
    "        results = processor.process_all()\n",
    "        \n",
    "        # Basic validation and statistics\n",
    "        for name, df in results.items():\n",
    "            if not df.empty:\n",
    "                logger.info(f\"\\nSummary for {name}:\")\n",
    "                logger.info(f\"Total records: {len(df)}\")\n",
    "                logger.info(f\"Missing values:\\n{df.isna().sum()}\")\n",
    "                logger.info(f\"Unique patients: {df['patient_id'].nunique()}\")\n",
    "            else:\n",
    "                logger.warning(f\"No valid records found for {name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
